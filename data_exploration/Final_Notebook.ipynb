{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import kaggle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from datetime import datetime\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelBinarizer, MultiLabelBinarizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from numpy import where\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score, recall_score\n",
    "from sklearn.model_selection import GridSearchCV, validation_curve\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import seaborn as sns\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding\n",
    "\n",
    "In the last decade, e-commerce has fundamentally changed how we live our lives through how we shop. Companies such as Sears have gone bankrupt over the years, making the transition from brick and mortar to an online e-commerce marketplace, however other companies such as Chewy, have been able to exploit e-commerce to become a market leader in their category.\n",
    "\n",
    "A study by emarketer.com found that the pandemic has had beneficial effects on US e-commerce. Sales will reach $794.50 billion this year, up 32.4% year-over-year. That’s a much higher growth rate than the 18.0% predicted in our Q2 forecast, as consumers continue to avoid stores and opt for online shopping amid the pandemic. By the end of the year e-commerce sales will reach 14.4% of all US retail spending for the year and 19.2% by 2024. If you further dig into the data and exclude gas and auto sales (categories sold almost exclusively offline), ecommerce penetration jumps to 20.6%.(1)\n",
    "\n",
    "With e-commerce growing at such an unprecedented rate, many companies are to capitalise on this change in consumer behaviour. It comes as no surprise to many that purchasing items online is a different process from buying an item in a store. While in a store an employee can help guide a customer to items they are both looking for and items they may want to consider purchasing, many e-commerce marketplaces dont have the same leverage; it's much easier to close out of a \"You should buy\" popup, rather than to ignore the advice of an instore expert. \n",
    "\n",
    "This has presented a serious challenge to e-commerce stores in the form of cart abandonment. Cart abandonment is when a customer leaves without buying, after adding an item to their cart. It is a trend that has remained steady since e-commerce entered the mainstream, as seen in the below chart from statista.\n",
    "\n",
    "<a href=\"https://www.statista.com/statistics/477804/online-shopping-cart-abandonment-rate-worldwide/\" rel=\"nofollow\"><img src=\"https://www.statista.com/graphic/1/477804/online-shopping-cart-abandonment-rate-worldwide.jpg\" alt=\"Statistic: Online shopping cart abandonment rate worldwide from 2006 to 2019 | Statista\" style=\"width: 100%; height: auto !important; max-width:1000px;-ms-interpolation-mode: bicubic;\"/></a><br />Find more statistics at  <a href=\"https://www.statista.com\" rel=\"nofollow\">Statista</a>\n",
    "\n",
    "The good news for data scientists is that during an in store purchase a customer can have a large degree of privacy, while almost every move an online customer makes is tracked and stored in a series of databases. This data can be used to produce insights into customer purchasing behaviour, and spending patterns. Using the \"eCommerce Events History in Cosmetics Shop\" on kaggle (3) https://www.kaggle.com/mkechinov/ecommerce-events-history-in-cosmetics-shop we plan to analyse the purchasing patterns in the five month history the dataset provides and predict wether a customer is going to remove an item from their cart. With this knowledge a company can then provide incentives, such as free shipping or discounts to turn that removal into a purchase.\n",
    "\n",
    "# Data Understanding\n",
    "\n",
    "The kaggle e-commerce dataset contains behavior data for five months (Oct 2019 – Feb 2020) from a medium sized unnamed cosmetics online store. Each row in the file represents an online event or action. All events are related to products and users.\n",
    "\n",
    "The dataset contains the following features:\n",
    "1. `event_time`: Time when event happened at (in UTC).\n",
    "2. `event_type`: What action a customer took. For more information please see below.\n",
    "3. `product_id`: ID of a product\n",
    "4. `category_id`: Product's category ID\n",
    "5. `category_code`: Product's category taxonomy (code name) if it was possible to make it. Usually present for meaningful categories and skipped for different kinds of accessories.\n",
    "6. `brand`: Downcased string of brand name. Can be a nan value, if it was missed.\n",
    "7. `price`: Float price of a product. Present.\n",
    "8. `user_id`: Permanent user ID.\n",
    "9. `user_session` :Temporary user's session ID. Same for each user's session. This value is changed every time user come back to online store from a long pause.\n",
    "\n",
    "\n",
    "`event_type` is further broken up into four components, these are:\n",
    "1. view - a user viewed a product\n",
    "2. cart - a user added a product to shopping cart\n",
    "3. remove_from_cart - a user removed a product from shopping cart\n",
    "4. purchase - a user purchased a product\n",
    "\n",
    "An example of a purchase funnel, may be three chronological rows, with the rows sharing `user_session` and `user_id` values. Where the first row is a view, the second is cart, and the final is a purchase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I set out to build a machine learning model that can predict whether an item will be purchased or removed from the cart on an e-commerce store. This will be used to help in marketing, sales. On the supply chain side it will also give more insights into how a product is viewed by customers and how much a brand/product should be stocked or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to read the CSVs from our data folder and turn them into Pandas DataFrames for functionality. For a list of instructions on how to download the data form kaggle please see \"../data/data_download.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert csv to DataFrame\n",
    "oct19 = pd.read_csv(\"../data/2019-Oct.csv\")\n",
    "nov19 = pd.read_csv(\"../data/2019-Nov.csv\")\n",
    "dec19 = pd.read_csv(\"../data/2019-Dec.csv\")\n",
    "jan20 = pd.read_csv(\"../data/2020-Jan.csv\")\n",
    "feb20 = pd.read_csv(\"../data/2020-Feb.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to combine each DataFrame that represents a month into one DataFrame that holds all the months data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecom_df = pd.concat([oct19, nov19, dec19, jan20, feb20], axis=0)\n",
    "print(ecom_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping columns here to both save memory latter on, also they are irrelevant to our data as we are only looking at purhcased, or removed from cart event_types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecom_df = ecom_df[ecom_df.event_type != 'view']\n",
    "ecom_df = ecom_df[ecom_df.event_type != 'cart']\n",
    "print(ecom_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start working with the data we need to make sure the dataset is in a workable state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renaming unknown values in the brand column with the string 'Unknown'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecom_df.brand.fillna(value=\"unknown\", axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping the remaining NA's in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecom_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidying the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping category_id, as we will use category code instead. It is redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecom_df.drop(\"category_id\", axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing any negative prices that appear in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecom_df = ecom_df[ecom_df['price']>= 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting our target variable, `event_type` to binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecom_df.event_type = ecom_df[\"event_type\"].replace({'purchase': 0, 'remove_from_cart': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecom_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our DataFrame doesn't have many feautres, we will have to build out our own features to help our models perform better. As we have a timestamp for every event, we will start with Feature Engineering using time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Based Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we convert our event_time into a datatime object to allow us to have more functionality with the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecom_df['event_time'] = pd.to_datetime(ecom_df['event_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first feature we will add, is if an event happened on a Holiday. It seems reasonable that people may be more inclined to spend money when they have the day off work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr = pd.date_range(start='2019-10-01', end='2020-02-29')\n",
    "df = pd.DataFrame()\n",
    "df['Date'] = dr\n",
    "\n",
    "cal = calendar()\n",
    "holidays = cal.holidays(start=dr.min(), end=dr.max())\n",
    "ecom_df['holiday'] = ecom_df['event_time'].isin(holidays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now will pull from the `event_time` column three new columns:\n",
    "1. What month was the event during?\n",
    "2. Was the event in 2020 or 2019?\n",
    "3. What hour of the day is the event taken place during?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecom_df['month'] = pd.DatetimeIndex(ecom_df['event_time']).month\n",
    "ecom_df['2020'] = pd.DatetimeIndex(ecom_df['event_time']).year\n",
    "ecom_df['hour'] = pd.DatetimeIndex(ecom_df['event_time']).hour\n",
    "\n",
    "#converting the year to binary\n",
    "ecom_df['2020'] = ecom_df['2020'].replace({2019: 0, 2020: 1})\n",
    "\n",
    "#converting month to string\n",
    "ecom_df['month'] = ecom_df['month'].replace({1: 'January', 2: 'Febuary', 10: 'October', 11: 'November', 12: 'December'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to add a feature, on what day of the week will a purchase take place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecom_df['day'] = ecom_df['event_time'].dt.dayofweek\n",
    "ecom_df['day'] = ecom_df['day'].replace({0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating four bins, we will also establish a feature that describes at what period of the day did an event happen:\n",
    "1. Night: Midnight -> 6am\n",
    "2. Morning: 6am -> Midday\n",
    "3. Afternoon: Midday -> 6pm \n",
    "4. Evening: 6pm ->Midnight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating new when dataframe\n",
    "when_df = pd.DataFrame({'hour':range(1, 25)})\n",
    "bins = [0,6,12,18,24]\n",
    "labels = ['Night', 'Morning','Afternoon','Evening']\n",
    "when_df['when'] = pd.cut(when_df['hour'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "#joining the new dataframe\n",
    "ecom_df = ecom_df.join(when_df['when'], on=\"hour\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ecom_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use LabelBinarizer and OneHotEncoder on several feature in the natives dataset, and several of the features we built above to create many new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Labeling\n",
    "encoder = LabelBinarizer()\n",
    "brand_labels = pd.DataFrame(encoder.fit_transform(ecom_df[['brand']]), columns=encoder.classes_, index=ecom_df.index)\n",
    "encoder2 = LabelBinarizer()\n",
    "productid_labels = pd.DataFrame(encoder2.fit_transform(ecom_df[[\"product_id\"]]), columns=encoder2.classes_, index=ecom_df.index)\n",
    "\n",
    "#OHE\n",
    "ohe = OneHotEncoder(sparse=False, drop='first')\n",
    "catco_labels = pd.DataFrame(ohe.fit_transform(ecom_df[['category_code']]), index=ecom_df.index, columns=ohe.get_feature_names(ecom_df[[\"category_code\"]].columns))\n",
    "ohe = OneHotEncoder(sparse=False, drop='first')\n",
    "day_labels = pd.DataFrame(ohe.fit_transform(ecom_df[['day']]), index=ecom_df.index, columns=ohe.get_feature_names(ecom_df[[\"day\"]].columns))\n",
    "ohe = OneHotEncoder(sparse=False, drop='first')\n",
    "month_labels = pd.DataFrame(ohe.fit_transform(ecom_df[['month']]), index=ecom_df.index, columns=ohe.get_feature_names(ecom_df[[\"month\"]].columns))\n",
    "ohe = OneHotEncoder(sparse=False, drop='first')\n",
    "when_labels = pd.DataFrame(ohe.fit_transform(ecom_df[['when']]), index=ecom_df.index, columns=ohe.get_feature_names(ecom_df[[\"when\"]].columns))\n",
    "\n",
    "#join the dataframes together\n",
    "ecom_df = ecom_df.join(brand_labels)\n",
    "ecom_df = ecom_df.join(productid_labels)\n",
    "ecom_df = ecom_df.join(catco_labels)\n",
    "ecom_df = ecom_df.join(day_labels)\n",
    "ecom_df = ecom_df.join(month_labels)\n",
    "ecom_df = ecom_df.join(when_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ecom_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now are data has been cleaned up and the new features we wanted to add have been built, we now need to establish what our target column is and set it to y, with our predictors being set to X.\n",
    "\n",
    "We will also need to drop our features in `ecom_df` that we have encoded or are redundant.\n",
    "\n",
    "As our `event_type` column has already been converted to binary we can set it to y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ecom_df.drop([\"month\", \"hour\", \"day\", \"when\", \"user_id\", \"product_id\", \"category_code\", \"event_time\", \"event_type\", \"brand\", \"user_session\"], axis=1)\n",
    "y = ecom_df.event_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also check out y classes to see if there is a class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecom_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there is a class imbalance. We will have to account for this before we model.\n",
    "\n",
    "Now that our data has been seperated we need to split the data into a train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have a class imbalance of roughly 2:1 in favour of our `removed_from_cart` category we will smote our training data to help our models perform more accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=42)\n",
    "X_tr_sm, y_tr_sm = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_sm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LGBM is one of the strongest, if not the strongest, model on the market currently. I have had success with it before and wanted to start with a strong model to see what kind of results it can give as a benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate XGBClassifier\n",
    "lgbm = lgb.LGBMClassifier()\n",
    "\n",
    "# Fit XGBClassifier\n",
    "lgbm.fit(X_tr_sm,  y_tr_sm)\n",
    "\n",
    "# Predict on training and test sets\n",
    "training_preds_lgbm = lgbm.predict(X_tr_sm)\n",
    "test_preds_lgbm = lgbm.predict(X_test)\n",
    "\n",
    "# Printing the Classification Report to see how our model performed\n",
    "first_results=classification_report(y_test, test_preds_lgbm)\n",
    "print(first_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While LGBM gave great results with default hyperparameters, I wanted to try some different models first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next model was a Decision Tree. We wanted something to compare to XGBoost that was a little bit more simple but still could deal with complex models. We also wanted a model that could execute faster than XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf = DecisionTreeClassifier() \n",
    "\n",
    "tree_clf.fit(X_tr_sm, y_tr_sm)\n",
    "\n",
    "# Predict on training and test sets\n",
    "training_preds_tclf = tree_clf.predict(X_tr_sm)\n",
    "test_preds_tclf = tree_clf.predict(X_test)\n",
    "\n",
    "# Printing the Classification Report to see how our model performed\n",
    "first_results=classification_report(y_test, test_preds_tclf)\n",
    "print(first_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While RandomForest usually performs worse than LGBM, I wanted to do my due dilligence and test the models performance first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf.fit(X_tr_sm, y_tr_sm)\n",
    "\n",
    "# Predict on training and test sets\n",
    "training_preds_rf = rf.predict(X_tr_sm)\n",
    "test_preds_rf = rf.predict(X_test)\n",
    "\n",
    "# Printing the Classification Report to see how our model performed\n",
    "first_results=classification_report(y_test, test_preds_rf)\n",
    "print(first_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparamter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have established that our RandomForestClassifier appears to be the best performing model at baseline. The next step is to adjust certain parameters to help improve the quality of our model. \n",
    "\n",
    "Warning: this grid will take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 95, 110, 115, 120],\n",
    "    'max_features': [2, 3, 7, 10, 30, 50, 100, 150],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'n_estimators': [100, 200, 300, 400]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the grid search model\n",
    "clf = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)\n",
    "clf.fit(X_tr_sm, y_tr_sm)\n",
    "GridSearchCV(estimator=rf,\n",
    "             param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_features, test_labels):\n",
    "    predictions = model.predict(test_features)\n",
    "    errors = abs(predictions - test_labels)\n",
    "    mape = 100 * np.mean(errors / test_labels)\n",
    "    accuracy = 100 - mape\n",
    "    print('Model Performance')\n",
    "    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "    print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the grid search to the data\n",
    "print(clf.best_params_)\n",
    "best_grid = clf.best_estimator_\n",
    "grid_accuracy = evaluate(best_grid, X_test, y_test)\n",
    "print(grid_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the param_grid in our HyperParameter step, we established that the best model is as follows:\n",
    "RandomForestClassifier with:\n",
    "1. criterion: gini\n",
    "2. max_depth: 115\n",
    "3. max_features: 100\n",
    "4. n_estimators: 400\n",
    "5. bootstrap: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best_model = RandomForestClassifier(bootstrap= True, criterion= 'gini', max_depth= 115, max_features= 100, n_estimators= 400)\n",
    "\n",
    "Best_model.fit(X_tr_sm, y_tr_sm)\n",
    "\n",
    "# Predict on training and test sets\n",
    "training_preds_bm = Best_model.predict(X_tr_sm)\n",
    "test_preds_bm = Best_model.predict(X_test)\n",
    "\n",
    "# Printing a classifcation report for our best model\n",
    "print(classification_report(y_tr_sm, training_preds_bm))\n",
    "print(classification_report(y_test, test_preds_bm))\n",
    "\n",
    "# Saving y preds train\n",
    "y_pred_tr = pd.DataFrame(data=y_pred_best, columns=y_train.columns, index=y_train.index)\n",
    "y_pred_tr.to_csv('../data/best_y_preds.csv')\n",
    "\n",
    "# Saving y preds test\n",
    "training_preds_bm = pd.DataFrame(data=test_preds_bm, columns=y_test.columns, index=y_test.index)\n",
    "test_preds_bm.to_csv('../data/y_preds_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "plot_confusion_matrix(estimator=Best_model, y_true=y_tr_sm, X=X_tr_sm)\n",
    "plt.savefig(\"conufsion_matrix_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "plot_confusion_matrix(estimator=Best_model, y_true=y_test, X=X_test)\n",
    "plt.savefig(\"conufsion_matrix_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (oy-env)",
   "language": "python",
   "name": "oy-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
